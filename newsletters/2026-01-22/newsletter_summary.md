- **[GLM-4.7-Flash 成为本地推理新宠](https://huggingface.co/zai-org/GLM-4.7-Flash)**（来源：AINews）
  > 30B-A3B MoE模型，支持200K上下文，在SWE-Bench等基准表现优异，可在24GB内存机器上本地运行，已集成至llama.cpp和Ollama生态。

- **[Google AI 揭示推理模型的“思想社会”机制](https://twitter.com/rohanpaul_ai/status/2013431689889095767)**（来源：AINews）
  > 研究发现推理模型性能提升源于内部出现类似辩论的模式（质疑、探索、分歧、收敛），这种机制能带来超过20%的准确率优势。

- **[合成推理数据研究：在相同计算预算下，“更多采样”优于“更大模型”](https://twitter.com/LiorOnAI/status/2013582631124771104)**（来源：AINews）
  > DeepMind研究总结，用更小模型生成更多合成推理数据，比用更大模型生成更少数据效果更好，训练收益最高达31.6%。

- **[microsoft/agent-lightning](https://github.com/microsoft/agent-lightning)**（来源：GitHub Trending）
  > 微软开源的AI智能体训练框架，无需修改代码即可通过强化学习、自动提示优化等方法提升现有智能体性能，兼容主流框架。

- **[Show HN：Grov——AI编程代理的多玩家协作平台](https://news.ycombinator.com/item?id=46711958)**（来源：Hacker News）
  > 开源AI编程助手上下文层，为团队AI代理提供共享持久记忆，记录决策原因并支持分支管理，优化token使用。

- **[Show HN：Infinate——O(k)常数时间空间注意力，实现无限LLM上下文](https://news.ycombinator.com/item?id=46711766)**（来源：Hacker News）
  > 开源注意力机制，利用3D语义空间和最近邻限制实现O(k)常数复杂度，在CPU上处理千万级token仅需7-14毫秒，内存恒定1.5MB。

- **[Ask HN：你在AI智能体上下文工程中遇到的最大挑战是什么？](https://news.ycombinator.com/item?id=46707675)**（来源：Hacker News）
  > AI代理开发中，上下文管理成为主要瓶颈，包括调试决策依据、多代理间上下文协调及历史存储优化。

- **[Anthropic 开源其原始家庭作业任务](https://news.ycombinator.com/item?id=46700594)**（来源：Hacker News）
  > 开源其性能优化面试题，涉及为虚构的VLIW SIMD CPU架构优化无文档的Python程序，是理解底层优化的绝佳案例。

- **[三类LLM工作负载及其服务策略](https://news.ycombinator.com/item?id=46707708)**（来源：Hacker News）
  > 讨论LLM工作负载分类（交互式、批处理、流式）及对应的服务策略，为后端工程师设计AI服务架构提供参考。

- **[软删除的挑战](https://news.ycombinator.com/item?id=46698061)**（来源：Hacker News）
  > 深入探讨数据库软删除在数据一致性、查询性能、存储膨胀和级联删除等方面带来的工程挑战与解决方案。
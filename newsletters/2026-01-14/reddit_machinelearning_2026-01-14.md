## Reddit ML - 2026-01-14


### 1. [[D] 试图解释混合专家模型的人，比真正构建它的人还多](https://www.reddit.com/r/MachineLearning/comments/1qbu8wp/d_i_see_more_people_trying_to_explain_mhc_than/)
> 用户抱怨关于mHC的在线解释过多，但实际可用的代码实现极少，希望获得可直接集成到项目中的实用资源。

<sub>作者: /u/Affectionate_Use9936 | 发布于: 2026-01-13 15:27</sub>

---

### 2. [[研究]（DeepSeek）通过可扩展查找实现条件记忆：大语言模型稀疏性的新维度](https://www.reddit.com/r/MachineLearning/comments/1qbnkrn/r_deepseek_conditional_memory_via_scalable_lookup/)
> DeepSeek提出Engram模块，为Transformer引入条件内存机制，通过O(1)查找优化知识检索，在多项推理和代码任务上超越传统MoE模型。

<sub>作者: /u/Nunki08 | 发布于: 2026-01-13 10:07</sub>

---

### 3. [[R] 带自蒸馏寄存器的视觉Transformer，NeurIPS 2025](https://www.reddit.com/r/MachineLearning/comments/1qbtbfb/r_vision_transformers_with_selfdistilled/)
> 提出一种低成本方法PH-Reg，无需重新训练即可改进现有视觉Transformer的密集特征，提升分割和深度估计性能。

<sub>作者: /u/44seconds | 发布于: 2026-01-13 14:51</sub>

---

### 4. [[D] 为什么因果关系对生产级机器学习至关重要：超越相关性](https://www.reddit.com/r/MachineLearning/comments/1qbkkxz/d_why_causality_matters_for_production_ml_moving/)
> 作者基于多年经验指出，模型常因学习相关性而非因果机制而在生产中失败，并开始撰写系列文章探讨因果机器学习系统。

<sub>作者: /u/KelynPaul | 发布于: 2026-01-13 06:57</sub>

---

### 5. [[P] 超强物理AI资源集——精选学术论文与资料库，聚焦视觉语言动作模型、世界模型、具身智能与机器人基础模型。](https://www.reddit.com/r/MachineLearning/comments/1qc6ybk/p_awesome_physical_ai_a_curated_list_of_academic/)
> 作者整理了一个关于物理AI（基础模型与机器人结合）的学术资源清单，涵盖模型、架构、部署等多个方面，并开放贡献。

<sub>作者: /u/kwk236 | 发布于: 2026-01-13 23:24</sub>

---

### 6. [[P] 为LLM实现语义缓存远比看起来困难——这是我们的经验总结](https://www.reddit.com/r/MachineLearning/comments/1qc0oll/p_semantic_caching_for_llms_is_way_harder_than_it/)
> Bifrost分享了在网关中构建语义缓存的架构与关键决策，包括双层匹配、异步嵌入生成和应对边缘情况，代码已开源。

<sub>作者: /u/dinkinflika0 | 发布于: 2026-01-13 19:27</sub>

---

### 7. [[D] 真的有人花钱请人做GPU集群总拥有成本咨询吗？（因为大多数公司多付了20%以上）](https://www.reddit.com/r/MachineLearning/comments/1qbljgq/d_is_anyone_actually_paying_for_gpu_cluster_tco/)
> 作者指出企业采购AI基础设施时仅关注GPU时租价是误区，真实成本需考虑性能效率、隐藏费用和上市速度，并提议提供TCO咨询服务。

<sub>作者: /u/New_Friendship9113 | 发布于: 2026-01-13 07:56</sub>

---

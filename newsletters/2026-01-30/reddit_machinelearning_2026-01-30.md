## Reddit ML - 2026-01-30


### 1. [[R] AlphaGenome：DeepMind统一DNA序列模型，以单碱基分辨率预测11种模式的调控变异效应（《自然》2026）](https://www.reddit.com/r/MachineLearning/comments/1qq4lnc/r_alphagenome_deepminds_unified_dna_sequence/)
> DeepMind发布AlphaGenome模型，能根据DNA序列预测基因组功能，性能优于多数专业模型，且训练和推理速度更快。

<sub>作者: /u/Fair-Rain3366 | 发布于: 2026-01-29 10:02</sub>

---

### 2. [[R] 知识图谱即隐式奖励模型：路径推导信号实现组合推理 —— 我们提出将知识图谱作为可扩展奖励模型，以支持组合推理的研究论文](https://www.reddit.com/r/MachineLearning/comments/1qq4sn4/r_knowledge_graphs_are_implicit_reward_models/)
> 研究提出用知识图谱作为奖励模型，为AI推理步骤提供可验证的奖励，使较小模型在组合推理任务上超越更大模型。

<sub>作者: /u/kyuval | 发布于: 2026-01-29 10:13</sub>

---

### 3. [[P] 视频高光时刻提取器](https://www.reddit.com/r/MachineLearning/comments/1qqfl8x/p_videohighlighter/)
> 介绍一款免费视频高亮工具，可基于场景、动作、物体和音频自动生成亮点与字幕。

<sub>作者: /u/Aseiel | 发布于: 2026-01-29 17:49</sub>

---

### 4. [[D] 实践中依赖G-CTR式保证的教训总结](https://www.reddit.com/r/MachineLearning/comments/1qq0xcl/d_lessons_learned_when_trying_to_rely_on/)
> 研究AI评估与静态保证的局限性，发现静态保证可能忽视自适应故障，基准测试不等于部署信心，过度优化指标会掩盖某些失败案例。

<sub>作者: /u/Obvious-Language4462 | 发布于: 2026-01-29 06:23</sub>

---

### 5. [【研究】通过对比分析评估代码环境中的奖励机制漏洞检测](https://www.reddit.com/r/MachineLearning/comments/1qqhqi2/r_benchmarking_reward_hack_detection_in_code/)
> 研究提出TRACE基准测试，发现对比性异常检测比孤立分类更能有效识别代码生成中的奖励攻击，GPT-5.2检测率提升至63%。

<sub>作者: /u/Megixist | 发布于: 2026-01-29 19:04</sub>

---

### 6. [[讨论] ICML投稿政策类型](https://www.reddit.com/r/MachineLearning/comments/1qq3rzb/d_icml_submission_policy_type/)
> ICML 2026审稿将采用两种LLM使用政策：A政策严格禁止，B政策允许用LLM辅助理解论文和润色审稿，但禁止让其评估优缺点或撰写全文。

<sub>作者: /u/Ok-Internet-196 | 发布于: 2026-01-29 09:12</sub>

---

### 7. [[p] Kaggleingest —— 为LLM竞赛导入数据集架构与相关笔记](https://www.reddit.com/r/MachineLearning/comments/1qq0sly/p_kaggleingest_ingest_dataset_schema_and/)
> 介绍一个受gitingest启发的个人项目kaggleingest，用于处理数据集模式和数据。

<sub>作者: /u/Low-Mastodon-4291 | 发布于: 2026-01-29 06:16</sub>

---

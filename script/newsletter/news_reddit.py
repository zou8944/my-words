"""
Reddit é¢‘é“ RSS æºè·å–
é¢‘é“ RSS æºæ ¼å¼: https://www.reddit.com/r/{channel}/top/.rss

åŒ…å«ä»¥ä¸‹é¢‘é“ï¼š
- å¹æ°´ç±»:
  - AMA - Ask Me Anything
  - AskReddit
  - Showerthoughts
  - todayilearned
- æŠ€æœ¯ç±»:
  - devops
  - programming
  - explainlikeimfive
  - golang
  - rust
  - MachineLearning
"""

from datetime import datetime
from typing import Optional

import llm
import news_utils

logger = news_utils.setup_logger(__name__)


def concurrent_translate_title(titles: list[str]) -> list[Optional[str]]:
    system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ Reddit å¸–å­ç¿»è¯‘ä¸“å®¶ï¼Œè´Ÿè´£å°†è‹±æ–‡å¸–å­æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ã€‚

## ä»»åŠ¡ç›®æ ‡ï¼š
å°†ä»¥ä¸‹ Reddit çš„å¸–å­æ ‡é¢˜ç¿»è¯‘æˆç®€æ´æµç•…çš„ä¸­æ–‡æ ‡é¢˜ã€‚

## è¾“å‡ºè¦æ±‚ï¼š
- **è¯­è¨€é£æ ¼**ï¼šè‡ªç„¶ã€ç®€æ´ã€æ˜“æ‡‚
- **æ ¼å¼è¦æ±‚**ï¼šçº¯æ–‡æœ¬ï¼Œä¸ä½¿ç”¨Markdown
- ä¿æŒåŸæ ‡é¢˜çš„è¯­æ°”å’Œé£æ ¼ï¼ˆæé—®ã€é™ˆè¿°ã€åˆ†äº«ç­‰ï¼‰
"""

    user_prompts = [
        f"""è¯·å°†ä»¥ä¸‹è‹±æ–‡æ ‡é¢˜ç¿»è¯‘æˆä¸­æ–‡ï¼š

---
è‹±æ–‡æ ‡é¢˜ï¼š
{title}
---

å¼€å§‹ç¿»è¯‘ï¼š"""
        for title in titles
    ]

    return llm.concurrent_one_shoot([(system_prompt, user_prompt) for user_prompt in user_prompts])


def concurrent_summarize_content(contents: list[str]) -> list[Optional[str]]:
    system_prompt = """ä½ æ˜¯ä¸€ä½èµ„æ·±çš„ Reddit å†…å®¹åˆ†æå¸ˆï¼Œä¸“é—¨å¤„ç† Reddit å¸–å­çš„æ¦‚è§ˆå†…å®¹ã€‚

## ä»»åŠ¡ç›®æ ‡ï¼š
å¯¹ Reddit å¸–å­çš„ RSS æ¦‚è§ˆè¿›è¡Œæ™ºèƒ½åˆ†æå’Œæ€»ç»“ã€‚

## å¤„ç†è§„åˆ™ï¼š
1. **å†…å®¹åˆ¤æ–­**ï¼š
   - å¦‚æœå†…å®¹åªåŒ…å«é“¾æ¥ï¼ˆé€šå¸¸å°‘äº50å­—ç¬¦ä¸”ä¸»è¦æ˜¯URLï¼‰ï¼Œç›´æ¥è¿”å›"æ— æ‘˜è¦"
   - å¦‚æœå†…å®¹åŒ…å«å®è´¨æ€§æè¿°æˆ–ä»‹ç»ï¼Œè¿›è¡Œæ€»ç»“

2. **æ€»ç»“è¦æ±‚**ï¼š
   - **å­—æ•°é™åˆ¶**ï¼šä¸¥æ ¼æ§åˆ¶åœ¨50å­—ä»¥å†…
   - **è¯­è¨€é£æ ¼**ï¼šç®€æ´ã€å‡†ç¡®ã€æ˜“æ‡‚
   - **æ ¼å¼è¦æ±‚**ï¼šçº¯æ–‡æœ¬ï¼Œä¸ä½¿ç”¨Markdown
   - **å†…å®¹é‡ç‚¹**ï¼šæå–æ ¸å¿ƒè§‚ç‚¹å’Œå…³é”®ä¿¡æ¯

## è´¨é‡æ ‡å‡†ï¼š
- å¿½ç•¥æ— å…³çš„å…ƒæ•°æ®ï¼ˆç‚¹èµæ•°ã€è¯„è®ºæ•°ç­‰ï¼‰
- ä¸“æ³¨äºæ ¸å¿ƒä»·å€¼å’Œå®ç”¨ä¿¡æ¯
- ä¿æŒå®¢è§‚ä¸­ç«‹çš„è¡¨è¿°
- å¦‚æœå†…å®¹è´¨é‡ä¸é«˜æˆ–ä¿¡æ¯ä¸è¶³ï¼Œç›´æ¥è¿”å›"æ— æ‘˜è¦"

è¯·æ ¹æ®ä»¥ä¸Šè§„åˆ™è¿›è¡Œå¤„ç†ã€‚"""

    user_prompts = [
        f"""è¯·åˆ†æä»¥ä¸‹ Reddit å¸–å­æ¦‚è§ˆå†…å®¹ï¼š

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ“‹ æ¦‚è§ˆå†…å®¹ï¼š
{content}
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

å¤„ç†è¦æ±‚ï¼š
1. åˆ¤æ–­å†…å®¹æ˜¯å¦åªåŒ…å«é“¾æ¥ï¼ˆå¦‚æœæ˜¯ï¼Œè¿”å›"æ— æ‘˜è¦"ï¼‰
2. å¦‚æœæœ‰å®è´¨å†…å®¹ï¼Œç”¨50å­—ä»¥å†…æ€»ç»“æ ¸å¿ƒè¦ç‚¹
3. ä¸“æ³¨äºæ ¸å¿ƒä»·å€¼å’Œå…³é”®ä¿¡æ¯
4. ä½¿ç”¨ç®€æ´çš„ä¸­æ–‡è¡¨è¾¾

å¼€å§‹åˆ†æï¼š"""
        for content in contents
    ]

    return llm.concurrent_one_shoot([(system_prompt, user_prompt) for user_prompt in user_prompts])


def process_reddit_channel(rss_url: str, slug: str, file_title: str):
    logger.info(f"å¼€å§‹è·å– Reddit é¢‘é“: {slug}")
    entries = news_utils.get_rss_entries(rss_url, 20)

    for entry in entries:
        entry["summary_md"] = news_utils.convert_html_to_markdown(entry.get("summary", "æ— æ‘˜è¦"))

    logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† Reddit {slug} é¢‘é“æ ‡é¢˜ç¿»è¯‘")
    translated_titles = concurrent_translate_title([entry.get("title", "æ— æ ‡é¢˜") for entry in entries])
    logger.info(f"å¼€å§‹å¹¶å‘å¤„ç† Reddit {slug} é¢‘é“æ‘˜è¦ç¿»è¯‘")
    summaries = concurrent_summarize_content([entry.get("summary_md", "æ— æ‘˜è¦") for entry in entries])

    current_date = datetime.now().strftime("%Y-%m-%d")

    final_contents = [f"## {file_title} - {current_date}\n\n"]
    for index, entry in enumerate(entries):
        translated_title = translated_titles[index] if translated_titles[index] else "æ— æ ‡é¢˜"
        link = entry.get("link", "æ— é“¾æ¥")
        translated_summary = summaries[index] if summaries[index] else "æ— æ‘˜è¦"
        translated_summary_md = translated_summary.replace("\n", "\n> ")  # type: ignore
        author = entry.get("author", "æ— ä½œè€…")
        publish_date_str = news_utils.get_entry_datetime_formated(entry)

        if len(translated_summary_md) < 5:
            final_contents.append(
                f"### {index + 1}. [{translated_title}]({link})\n"
                f"\n<sub>ä½œè€…: {author} | å‘å¸ƒäº: {publish_date_str}</sub>\n\n"
                f"---\n"
            )
        else:
            final_contents.append(
                f"### {index + 1}. [{translated_title}]({link})\n"
                f"> {translated_summary_md}\n"
                f"\n<sub>ä½œè€…: {author} | å‘å¸ƒäº: {publish_date_str}</sub>\n\n"
                f"---\n"
            )

    filename = get_today_news_file(slug)
    final_content = "\n".join(final_contents)
    if news_utils.put_file_to_r2_with_today(filename, final_content):
        logger.info(f"âœ“ {file_title} é¢‘é“å†…å®¹å·²ä¿å­˜åˆ° R2: {filename}")
    else:
        logger.error(f"âœ— æ— æ³•ä¿å­˜ {file_title} é¢‘é“å†…å®¹åˆ° R2: {filename}")


def get_today_news_file(slug: str) -> str:
    return f"{slug}_{news_utils.current_date_formatted()}.md"


def all_reddit_channels() -> list[tuple[str, str, str]]:
    """è¿”å›æ‰€æœ‰ Reddit é¢‘é“é…ç½®: (slug, rss_url, title)"""
    channels = []

    # å¹æ°´ç±»
    casual_channels = [
        ("AMA", "Reddit AMA"),
        ("AskReddit", "Reddit AskReddit"),
        ("Showerthoughts", "Reddit Showerthoughts"),
        ("todayilearned", "Reddit TIL"),
    ]

    # æŠ€æœ¯ç±»
    tech_channels = [
        ("devops", "Reddit DevOps"),
        ("programming", "Reddit Programming"),
        ("explainlikeimfive", "Reddit ELI5"),
        ("golang", "Reddit Golang"),
        ("rust", "Reddit Rust"),
        ("MachineLearning", "Reddit ML"),
    ]

    for channel, title in casual_channels + tech_channels:
        slug = f"reddit_{channel.lower()}"
        rss_url = f"https://www.reddit.com/r/{channel}/top/.rss"
        channels.append((slug, rss_url, title))

    return channels


def get_today_news_content():
    """è·å–æ‰€æœ‰ Reddit é¢‘é“çš„ä»Šæ—¥å†…å®¹"""
    for slug, url, title in all_reddit_channels():
        filename = get_today_news_file(slug)
        _content = news_utils.get_file_from_r2_with_today(filename)
        if _content:
            logger.info(f"ä»Šå¤©çš„ {title} é¢‘é“å†…å®¹å·²å­˜åœ¨ï¼Œè·³è¿‡: {filename}")
            continue

        process_reddit_channel(url, slug, title)


if __name__ == "__main__":
    get_today_news_content()
